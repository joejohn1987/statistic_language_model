{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  Building a simple statistical language model on a shakespeare courpus\n",
    "\n",
    "\n",
    "This is a simple python code to show how to build a n-gram language model.\n",
    "Here are the steps:\n",
    "\n",
    "1. Preprocessing\n",
    "      1. Get raw text\n",
    "      1. Tokenize the text\n",
    "      1. Clean the text\n",
    "2. Building the n-gram lm (language model)\n",
    "      1. get a sequence of word\n",
    "      2. get the frequency of the n-grams.\n",
    "      3. Loop through the whole corpus, store and compute the possibility of the existing n-grams.\n",
    "3. Use the model to complete a task, such as text generation\n",
    "      1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Let's start now.\n",
    "The first thing is get text from a txt file. One thing should be concerned is the encoding. \n",
    "Since we use python 3, make sure the input file is encoded in unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# store the file line by lines\n",
    "def get_raw_text(path):\n",
    "    with open(path) as f:\n",
    "        return f.readlines()\n",
    "raw_text = get_raw_text(\"data/train_shakespeare.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence! home, you idle creatures get you home: Is this a holiday? what! know you not, Being mechanical, you ought not walk Upon a labouring day without the sign Of your profession? Speak, what trade art thou?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let see the first line \n",
    "print(raw_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Looks nice!\n",
    "Then we can do the tokenization. For English it is rather a simple task because words are seperated by spaces.\n",
    "However, the punctuation is sticked to the previous word, you should be careful about that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "# Notice that at the end of line we have a newline character, so we have to remove it first by strip()\n",
    "def split_by_space(text):\n",
    "    return text.strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hence!', 'home,', 'you', 'idle', 'creatures', 'get', 'you', 'home:', 'Is', 'this', 'a', 'holiday?', 'what!', 'know', 'you', 'not,', 'Being', 'mechanical,', 'you', 'ought', 'not', 'walk', 'Upon', 'a', 'labouring', 'day', 'without', 'the', 'sign', 'Of', 'your', 'profession?', 'Speak,', 'what', 'trade', 'art', 'thou?']\n"
     ]
    }
   ],
   "source": [
    "# let see how it works\n",
    "print(split_by_space(raw_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We don't want the punctuation stick to the word, so lets seperate it from the word\n",
    "\n",
    "\n",
    "def split_punc(tokens):\n",
    "    output = []\n",
    "    punc = '!'\n",
    "    for token in tokens:\n",
    "        for i in range(len(token),0,-1):\n",
    "            if token[:i].isalpha():\n",
    "                if i==len(token):\n",
    "                    output.append(token)\n",
    "                else:\n",
    "                    output.append(token[:i])\n",
    "                    output.append(token[i:])\n",
    "                break\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hence', '!', 'home', ',', 'you', 'idle', 'creatures', 'get', 'you', 'home', ':', 'Is', 'this', 'a', 'holiday', '?', 'what', '!', 'know', 'you', 'not', ',', 'Being', 'mechanical', ',', 'you', 'ought', 'not', 'walk', 'Upon', 'a', 'labouring', 'day', 'without', 'the', 'sign', 'Of', 'your', 'profession', '?', 'Speak', ',', 'what', 'trade', 'art', 'thou', '?']\n"
     ]
    }
   ],
   "source": [
    "# let see how it works\n",
    "print(split_punc(tokenize(raw_text[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Turn uppercase into lowercase\n",
    "\n",
    "def lowercase(tokens):\n",
    "    output = []\n",
    "    for token in tokens:\n",
    "        output.append(token.lower())\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hence', '!', 'home', ',', 'you', 'idle', 'creatures', 'get', 'you', 'home', ':', 'is', 'this', 'a', 'holiday', '?', 'what', '!', 'know', 'you', 'not', ',', 'being', 'mechanical', ',', 'you', 'ought', 'not', 'walk', 'upon', 'a', 'labouring', 'day', 'without', 'the', 'sign', 'of', 'your', 'profession', '?', 'speak', ',', 'what', 'trade', 'art', 'thou', '?']\n"
     ]
    }
   ],
   "source": [
    "print(lowercase(split_punc(tokenize(raw_text[0]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pack them together\n",
    "def tokenizer(raw_text):\n",
    "    tokenized_text = []\n",
    "    for line in raw_text:\n",
    "        tokenized_text.append(lowercase(split_punc(tokenize(line))))\n",
    "    return tokenized_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hence', '!', 'home', ',', 'you', 'idle', 'creatures', 'get', 'you', 'home', ':', 'is', 'this', 'a', 'holiday', '?', 'what', '!', 'know', 'you', 'not', ',', 'being', 'mechanical', ',', 'you', 'ought', 'not', 'walk', 'upon', 'a', 'labouring', 'day', 'without', 'the', 'sign', 'of', 'your', 'profession', '?', 'speak', ',', 'what', 'trade', 'art', 'thou', '?']\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = tokenizer(raw_text)\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Language Model\n",
    "Now we have a clean data to train the model, let's build a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class language_model():\n",
    "    # initial with no. of gram\n",
    "    def __init__(self, n):\n",
    "        self.n = n\n",
    "        self.start_sign = '<s>'\n",
    "        self.end_sign = '<e>'\n",
    "        # build a dictionary to record the frequency of next word\n",
    "        self.f_next = {}\n",
    "        # build a dictionary to record the normalized probability of next word\n",
    "        self.p_next = {}\n",
    "        self.word_type = set()\n",
    "        \n",
    "    # a function to pad a sentence with start sign and end sign    \n",
    "    def pad(self, tokens):\n",
    "        return [self.start_sign]*(self.n-1) + tokens + [self.end_sign]*(self.n-1)\n",
    "    \n",
    "    \n",
    "    # train a list of sentence    \n",
    "    def train(self, tokenized_text):\n",
    "        for tokens_seq in tokenized_text:\n",
    "            padded_tokens = self.pad(tokens_seq)\n",
    "            for i in range(len(padded_tokens)-self.n):\n",
    "                history = tuple(padded_tokens[i:i+self.n-1])\n",
    "                next_word = padded_tokens[i+self.n]\n",
    "                if next_word!='<e>':\n",
    "                    self.word_type.add(next_word)\n",
    "                if history in self.f_next.keys():\n",
    "                    try:\n",
    "                        self.f_next[history][next_word] += 1\n",
    "                    except:\n",
    "                        self.f_next[history][next_word] = 1\n",
    "                else:\n",
    "                    self.f_next[history]={}\n",
    "                    self.f_next[history][next_word] = 1\n",
    "    \n",
    "    # normalized the frequency\n",
    "    def normalized(self):\n",
    "        for history in self.p_next:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build a tri-gram model\n",
    "lm_n3 = language_model(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test it with two sentences\n",
    "lm_n3.train(tokenized_text[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('<s>', '<s>'): {'hence': 1, '!': 1, 'why': 1, ',': 1}, ('<s>', 'hence'): {'home': 1}, ('hence', '!'): {',': 1}, ('!', 'home'): {'you': 1}, ('home', ','): {'idle': 1}, (',', 'you'): {'creatures': 1, 'not': 1}, ('you', 'idle'): {'get': 1}, ('idle', 'creatures'): {'you': 1}, ('creatures', 'get'): {'home': 1}, ('get', 'you'): {':': 1}, ('you', 'home'): {'is': 1}, ('home', ':'): {'this': 1}, (':', 'is'): {'a': 1}, ('is', 'this'): {'holiday': 1}, ('this', 'a'): {'?': 1}, ('a', 'holiday'): {'what': 1}, ('holiday', '?'): {'!': 1}, ('?', 'what'): {'know': 1}, ('what', '!'): {'you': 1}, ('!', 'know'): {'not': 1}, ('know', 'you'): {',': 1}, ('you', 'not'): {'being': 1}, ('not', ','): {'mechanical': 1}, (',', 'being'): {',': 1}, ('being', 'mechanical'): {'you': 1}, ('mechanical', ','): {'ought': 1}, ('you', 'ought'): {'walk': 1}, ('ought', 'not'): {'upon': 1}, ('not', 'walk'): {'a': 1}, ('walk', 'upon'): {'labouring': 1}, ('upon', 'a'): {'day': 1}, ('a', 'labouring'): {'without': 1}, ('labouring', 'day'): {'the': 1}, ('day', 'without'): {'sign': 1}, ('without', 'the'): {'of': 1}, ('the', 'sign'): {'your': 1}, ('sign', 'of'): {'profession': 1}, ('of', 'your'): {'?': 1}, ('your', 'profession'): {'speak': 1}, ('profession', '?'): {',': 1}, ('?', 'speak'): {'what': 1}, ('speak', ','): {'trade': 1}, (',', 'what'): {'art': 1}, ('what', 'trade'): {'thou': 1}, ('trade', 'art'): {'?': 1}, ('art', 'thou'): {'<e>': 1}, ('thou', '?'): {'<e>': 1}, ('?', '<e>'): {'<e>': 1}, ('<s>', 'why'): {'sir': 1}, ('why', ','): {',': 1}, (',', 'sir'): {'a': 1}, ('sir', ','): {'carpenter': 1}, (',', 'a'): {'.': 1}, ('a', 'carpenter'): {'<e>': 1}, ('carpenter', '.'): {'<e>': 1}, ('.', '<e>'): {'<e>': 1}}\n"
     ]
    }
   ],
   "source": [
    "print(lm_n3.f_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text generator\n",
    "Now we have a language model to use, let's build a text generator which can generator shakespeare's script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import random to make random choices\n",
    "import random\n",
    "\n",
    "\n",
    "def generator(lm, init_word = 'hence', min_len=5, max_len=200):\n",
    "    output_text = [lm.start_sign]*(lm.n-1)\n",
    "    while init_word == None or not init_word.isalpha():\n",
    "        init_word = random.choice(list(lm.word_type))\n",
    "    output_text.append(init_word)\n",
    "    while max_len > len(output_text)  and len(output_text) <= max_len and output_text[-1]!=lm.end_sign:\n",
    "        history = tuple(output_text[-lm.n+1:])\n",
    "        try:\n",
    "            output_text.extend(random.choices(list(lm.f_next[history].keys()), lm.f_next[history].values()))\n",
    "        except:\n",
    "            random_words = random.choice(list(lm.f_next.keys()))\n",
    "            output_text.extend(random_words)\n",
    "            output_text.extend(random.choices(list(lm.f_next[random_words].keys()), lm.f_next[random_words].values()))\n",
    "            \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<s>', 'hence', 'then', 'things', 'more', ':', 'my', 'can', 'this', 'mangled', 'at', 'heaven', ';', 'shall', 'along', 'you', 'scarcely', 'bruise', 'this', 'affect', '.', '<e>']\n"
     ]
    }
   ],
   "source": [
    "text = generator(lm_n3)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks legit! Lets make it into human lanugae."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def detokenizer(token_seq):\n",
    "    output_string = ''\n",
    "    for token in token_seq:\n",
    "        if token != '<s>' and token != '<e>':\n",
    "            if output_string == '':\n",
    "                output_string = output_string + token.capitalize()\n",
    "            elif not token.isalpha():\n",
    "                output_string = output_string + token\n",
    "            else:\n",
    "                output_string = output_string + \" \" + token \n",
    "    if output_string[-1].is\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence then things more: my can this mangled at heaven; shall along you scarcely bruise this affect.\n"
     ]
    }
   ],
   "source": [
    "print(detokenizer(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it again with training the whole corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hence verona it mis in repair and no general for sinking them's weapons water thou reportest to outlive the i grows it? is your?, thine,-- fifty five talents greet the senseless with thee hither thy either you stay\n"
     ]
    }
   ],
   "source": [
    "raw_text = get_raw_text(\"data/train_shakespeare.txt\")\n",
    "tokenized_text = tokenizer(raw_text)\n",
    "lm_n3 = language_model(3)\n",
    "lm_n3.train(tokenized_text)\n",
    "text_tokens = generator(lm_n3)\n",
    "generated_text = detokenizer(text_tokens)\n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
